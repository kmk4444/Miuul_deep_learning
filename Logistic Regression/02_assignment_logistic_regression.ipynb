{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6J9TLVdhK5P5FxmzyiFbx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/Miuul_deep_learning/blob/main/02_assignment_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Gradients"
      ],
      "metadata": {
        "id": "atbGBscEp56s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Veri Setindeki İlk Ağırlık İçin İterasyon, Gradyan ve Loss Değişimlerini Gözlemlemek İstiyoruz.\n",
        "\n",
        "Aşağıdaki çıktıdaki sonuçları almak için uygun kodları yazmanız beklenmektedir.\n",
        "\n",
        "## Beklenen çıktı:\n",
        "\n",
        "```\n",
        "    Iteration  Feature 1 Gradient      Loss\n",
        "0           1            0.346963  0.693127\n",
        "1           2            0.250376  0.527516\n",
        "2           3            0.196769  0.441138\n",
        "3           4            0.163503  0.387700\n",
        "4           5            0.140597  0.350752\n",
        "..        ...                 ...       ...\n",
        "95         96            0.010033  0.108377\n",
        "96         97            0.009923  0.108025\n",
        "97         98            0.009816  0.107678\n",
        "98         99            0.009711  0.107337\n",
        "99        100            0.009608  0.107001\n",
        "\n",
        "[100 rows x 3 columns]\n",
        "```\n",
        "\n",
        "## İpuçları:\n",
        "\n",
        "\n",
        "- Modelleme işlemi ve diğer tüm işlemler içeriklerde olduğu gibi tüm değişkenler üzerinden yapılır.\n",
        "\n",
        "- Tek bir değişken için gradyan istediğimiz için sadece gradyan hesabının yapıldığı dw içerisinden şu şekilde bir seçim yapılması yeterli dw[0] böylece ilk değişkenin gradyan bilgisini tutmuş olacağız.\n",
        "\n",
        "- gradient_descent fonksiyonunun içinde döngü başlamadan önce aşağıdaki gibi bir sözlük tanımlamak işinize yarayabilir:\n",
        "\n",
        "- grad_tracking = {'Iteration': [], 'Feature 1 Gradient': [], 'Loss': []}\n",
        "\n",
        "Sonrasında iterasyonlar devam ederken yani for döngüsünün içindeyken grad_tracking sözlük içerisine ilgili iterasyondaki iterasyon bilgisini (epoch +1), ilk değişkenin gradyan bilgisini (dw[0]) ve loss bilgisini eklemeniz yeterli (loss)\n",
        "\n",
        "***Yani döngü içindeyken basit bir sözlüğe eleman ekleme işimiz olacak.***\n",
        "\n",
        "- Predict fonksiyonuna ihtiyacımız yok amacımız gradyanları, iterasyonları ve loss'u gözlemlemek.\n",
        "\n",
        "- gradient_descent fonksiyonun çıktısı sadece grad_tracking'i return etmeli.\n",
        "\n",
        "- gradient_descent fonksiyonunu çağırırken argumanlar şunlar olabilir: gradient_descent(X_train_scaled, y_train, lr=0.1, epochs=100)\n",
        "\n",
        "- çıktı çirkin olacaktır, bu sebeple çıktıyı tutup sonrasında bir dataframe'e çevirmek iyi olacaktır.\n",
        "\n",
        "\n",
        "## Kod taslağı aşağıda bırakılmıştır, düzenleme yapılması gereken yerler işaretlenmiştir:\n"
      ],
      "metadata": {
        "id": "uSdQHUAFp7qK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m-NXK71NpTkG",
        "outputId": "84980cfd-14a6-4fcf-9186-4166b3d3c279",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Iteration  Feature 1 Gradient      Loss\n",
            "0           1            0.346963  0.693127\n",
            "1           2            0.250376  0.527516\n",
            "2           3            0.196769  0.441138\n",
            "3           4            0.163503  0.387700\n",
            "4           5            0.140597  0.350752\n",
            "..        ...                 ...       ...\n",
            "95         96            0.010033  0.108377\n",
            "96         97            0.009923  0.108025\n",
            "97         98            0.009816  0.107678\n",
            "98         99            0.009711  0.107337\n",
            "99        100            0.009608  0.107001\n",
            "\n",
            "[100 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def compute_loss(y, y_pred):\n",
        "    epsilon = 1e-5\n",
        "    return -np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n",
        "\n",
        "def compute_gradients(X, y, y_pred):\n",
        "    return np.dot(X.T, (y_pred - y)) / len(y)\n",
        "\n",
        "def gradient_descent(X, y, lr=0.01, epochs=100):\n",
        "    weights = np.zeros(X.shape[1])\n",
        "    bias = 0\n",
        "\n",
        "    # grad_tracking burada tanımlanabilir.\n",
        "    grad_tracking = {'Iteration': [], 'Feature 1 Gradient': [], 'Loss': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        z = np.dot(X, weights) + bias\n",
        "        y_pred = sigmoid(z)\n",
        "        loss = compute_loss(y, y_pred)\n",
        "        dw = compute_gradients(X, y, y_pred)\n",
        "        db = np.mean(y_pred - y)\n",
        "\n",
        "        # gradyanları ve loss'u burada yakalıyoruz.\n",
        "        dw_feature_1 = dw[0]\n",
        "        grad_tracking['Iteration'].append(epoch + 1)\n",
        "        grad_tracking['Feature 1 Gradient'].append(dw_feature_1)\n",
        "        grad_tracking['Loss'].append(loss)\n",
        "\n",
        "\n",
        "\n",
        "        weights -= lr * dw\n",
        "        bias -= lr * db\n",
        "\n",
        "    return grad_tracking\n",
        "\n",
        "result=gradient_descent(X_train_scaled, y_train, lr=0.1, epochs=100)\n",
        "result_pd=pd.DataFrame(data=result)\n",
        "print(result_pd)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: gradient_descent Fonksiyonunun Çıktısını Kaydedip dataframe'e Çeviriniz.\n",
        "\n",
        "Gradient descent fonksiyonunu aşağıdaki 3 kombinasyonda çağırarak sonuçları dataframe'e çevirip yazdırınız.\n",
        "\n",
        "\n",
        "1. gradient_descent(X_train_scaled, y_train, lr=0.1, epochs=100)\n",
        "2. gradient_descent(X_train_scaled, y_train, lr=0.01, epochs=100)\n",
        "3. gradient_descent(X_train_scaled, y_train, lr=0.001, epochs=100)\n",
        "\n",
        "\n",
        "## Beklenen çıktılar:\n",
        "\n",
        "\n",
        "```\n",
        "    Iteration  Feature 1 Gradient      Loss\n",
        "0           1            0.346963  0.693127\n",
        "1           2            0.250376  0.527516\n",
        "2           3            0.196769  0.441138\n",
        "3           4            0.163503  0.387700\n",
        "4           5            0.140597  0.350752\n",
        "..        ...                 ...       ...\n",
        "95         96            0.010033  0.108377\n",
        "96         97            0.009923  0.108025\n",
        "97         98            0.009816  0.107678\n",
        "98         99            0.009711  0.107337\n",
        "99        100            0.009608  0.107001\n",
        "\n",
        "[100 rows x 3 columns]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "    Iteration  Feature 1 Gradient      Loss\n",
        "0           1            0.346963  0.693127\n",
        "1           2            0.336617  0.673916\n",
        "2           3            0.326640  0.655887\n",
        "3           4            0.317060  0.638961\n",
        "4           5            0.307888  0.623058\n",
        "..        ...                 ...       ...\n",
        "95         96            0.085450  0.259625\n",
        "96         97            0.084800  0.258520\n",
        "97         98            0.084160  0.257431\n",
        "98         99            0.083530  0.256357\n",
        "99        100            0.082909  0.255299\n",
        "\n",
        "[100 rows x 3 columns]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "    Iteration  Feature 1 Gradient      Loss\n",
        "0           1            0.346963  0.693127\n",
        "1           2            0.345928  0.691178\n",
        "2           3            0.344896  0.689242\n",
        "3           4            0.343867  0.687318\n",
        "4           5            0.342841  0.685405\n",
        "..        ...                 ...       ...\n",
        "95         96            0.265483  0.552141\n",
        "96         97            0.264805  0.551030\n",
        "97         98            0.264130  0.549925\n",
        "98         99            0.263458  0.548826\n",
        "99        100            0.262789  0.547733\n",
        "\n",
        "[100 rows x 3 columns]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "V3xGABuwoWUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_1 = gradient_descent(X_train_scaled, y_train, lr=0.1, epochs=100)\n",
        "result_pd_1 = pd.DataFrame(data=result_1)\n",
        "print(result_pd_1)\n",
        "print(\"----------------------------------\")\n",
        "\n",
        "result_2 = gradient_descent(X_train_scaled, y_train, lr=0.01, epochs=100)\n",
        "result_pd_2 = pd.DataFrame(data=result_2)\n",
        "print(result_pd_2)\n",
        "print(\"----------------------------------\")\n",
        "\n",
        "result_3 = gradient_descent(X_train_scaled, y_train, lr=0.001, epochs=100)\n",
        "result_pd_3 = pd.DataFrame(data=result_3)\n",
        "print(result_pd_3)"
      ],
      "metadata": {
        "id": "grgWf7IzoXrs",
        "outputId": "9f38de81-24a4-4bb8-ad7e-fc0ed3faaf86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Iteration  Feature 1 Gradient      Loss\n",
            "0           1            0.346963  0.693127\n",
            "1           2            0.250376  0.527516\n",
            "2           3            0.196769  0.441138\n",
            "3           4            0.163503  0.387700\n",
            "4           5            0.140597  0.350752\n",
            "..        ...                 ...       ...\n",
            "95         96            0.010033  0.108377\n",
            "96         97            0.009923  0.108025\n",
            "97         98            0.009816  0.107678\n",
            "98         99            0.009711  0.107337\n",
            "99        100            0.009608  0.107001\n",
            "\n",
            "[100 rows x 3 columns]\n",
            "----------------------------------\n",
            "    Iteration  Feature 1 Gradient      Loss\n",
            "0           1            0.346963  0.693127\n",
            "1           2            0.336617  0.673916\n",
            "2           3            0.326640  0.655887\n",
            "3           4            0.317060  0.638961\n",
            "4           5            0.307888  0.623058\n",
            "..        ...                 ...       ...\n",
            "95         96            0.085450  0.259625\n",
            "96         97            0.084800  0.258520\n",
            "97         98            0.084160  0.257431\n",
            "98         99            0.083530  0.256357\n",
            "99        100            0.082909  0.255299\n",
            "\n",
            "[100 rows x 3 columns]\n",
            "----------------------------------\n",
            "    Iteration  Feature 1 Gradient      Loss\n",
            "0           1            0.346963  0.693127\n",
            "1           2            0.345928  0.691178\n",
            "2           3            0.344896  0.689242\n",
            "3           4            0.343867  0.687318\n",
            "4           5            0.342841  0.685405\n",
            "..        ...                 ...       ...\n",
            "95         96            0.265483  0.552141\n",
            "96         97            0.264805  0.551030\n",
            "97         98            0.264130  0.549925\n",
            "98         99            0.263458  0.548826\n",
            "99        100            0.262789  0.547733\n",
            "\n",
            "[100 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Learning Rate Değiştikçe Loss Değerleri Nasıl Değişmektedir? Sebebi Nedir?\n",
        "\n",
        "Bir önceki soruda farklı learning rate değerleri kullandık, bu farklı learning rate değerleri neticesinde ortaya çıkan loss değerleri nasıl değişmektedir? Sebebi nedir? Açıklayınız.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HoTGU6FmpLi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Düşük bir öğrenme hızıyla model yavaş adımlarla eğitildiğinde, log loss fonksiyonu genellikle daha yumuşak bir şekilde azalır. Bu, modelin optimum noktaya daha yakın bir minimuma ulaşmasına olanak tanır ancak eğitim süresi daha uzun olabilir. Aşırı düşük öğrenme hızı, modelin log loss fonksiyonunda saplanıp kalmasına neden olabilir, bu da eğitim sürecini etkisiz hale getirebilir."
      ],
      "metadata": {
        "id": "roGJweltqJkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: İlk Update Kuralı Sonucu Yeni Ağırlık Değeri Nedir?\n",
        "\n",
        "```\n",
        "    Iteration  Feature 1 Gradient      Loss\n",
        "0           1            0.346963  0.693127\n",
        "1           2            0.336617  0.673916\n",
        "2           3            0.326640  0.655887\n",
        "3           4            0.317060  0.638961\n",
        "4           5            0.307888  0.623058\n",
        "..        ...                 ...       ...\n",
        "95         96            0.085450  0.259625\n",
        "96         97            0.084800  0.258520\n",
        "97         98            0.084160  0.257431\n",
        "98         99            0.083530  0.256357\n",
        "99        100            0.082909  0.255299\n",
        "\n",
        "[100 rows x 3 columns]\n",
        "```\n",
        "\n",
        "\n",
        "## Elimizde şunlar var:\n",
        "\n",
        "- learning rate: 0.001\n",
        "- birinci iterasyondaki gradyan değeri: 0.346963\n",
        "- Ağırlığın başlangıç değeri: 0\n",
        "\n",
        "\n",
        "Bu durumda ilk update kuralı sonucu yeni ağırlık değeri ne olur?"
      ],
      "metadata": {
        "id": "uON4qLTuqMMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights= 0\n",
        "grads = 0.346963\n",
        "lr = 0.001"
      ],
      "metadata": {
        "id": "rV3Zhi_xpMbG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_weights = weights -lr * grads\n",
        "print(new_weights)"
      ],
      "metadata": {
        "id": "87fSNVfLqqQC",
        "outputId": "15dcff5e-bf5f-42e7-a4bc-b328bbec303a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.00034696300000000005\n"
          ]
        }
      ]
    }
  ]
}
