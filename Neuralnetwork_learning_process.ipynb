{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIZUrKWWki6/rRJaoIj9zk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/Miuul_deep_learning/blob/main/Neuralnetwork_learning_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "XN3GFS2ahtjV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ufql_7IGfFaI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import log_loss, accuracy_score, roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "4jPzF5ovi1E3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(filepath):\n",
        "  data = pd.read_csv(filepath)\n",
        "  scaler = StandardScaler()\n",
        "  X = scaler.fit_transform(data.drop(\"Outcome\", axis=1))\n",
        "  y = data[\"Outcome\"].values\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "B27MCAi0i21m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions"
      ],
      "metadata": {
        "id": "v7zaq3uJjKwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "#ReLu\n",
        "def relu(z):\n",
        "  return np.maximum(0, z)"
      ],
      "metadata": {
        "id": "N5QoJKHojMSH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward Propagation"
      ],
      "metadata": {
        "id": "3K0Xz3KFlkMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters):\n",
        "  W1, b1, W2, b2 = parameters\n",
        "\n",
        "  Z1 = np.dot(W1, X.T) + b1\n",
        "  A1 = relu(Z1)\n",
        "\n",
        "  Z2 = np.dot(W2, A1) + b2\n",
        "  A2 = sigmoid(Z2)\n",
        "\n",
        "  cache = (Z1, A1, Z2, A2)\n",
        "  return A2, cache"
      ],
      "metadata": {
        "id": "U4O0pH7tlnJZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back Propagation"
      ],
      "metadata": {
        "id": "pL6Bb996mBWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(X, Y, cache, parameters):\n",
        "  Z1, A1, Z2, A2 = cache\n",
        "  W1, b1, W2, b2 = parameters\n",
        "  m = X.shape[0]\n",
        "\n",
        "  dA2 = A2 - Y\n",
        "  dW2 = np.dot(dA2, A1.T) / m\n",
        "  db2 = np.sum(dA2, axis=1, keepdims=True) / m\n",
        "\n",
        "  dZ1 = np.dot(W2.T, dA2) * relu(Z1>0)\n",
        "  dW1 = np.dot(dZ1, X) / m\n",
        "  db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
        "\n",
        "  gradients = (dW1, db1, dW2, db2)\n",
        "  return gradients"
      ],
      "metadata": {
        "id": "fNA-zSRZmAst"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Update Parameters"
      ],
      "metadata": {
        "id": "JkjSErrMszhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "\n",
        "  W1, b1, W2, b2 = parameters\n",
        "  dW1, db1, dW2, db2 = gradients\n",
        "\n",
        "  W1 -= learning_rate * dW1\n",
        "  b1 -= learning_rate * db1\n",
        "  W2 -= learning_rate * dW2\n",
        "  b2 -= learning_rate * db2\n",
        "\n",
        "  return (W1, b1, W2, b2)"
      ],
      "metadata": {
        "id": "_9NBJ82ws1ci"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Evaluate"
      ],
      "metadata": {
        "id": "1z6N6utatHv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(X_train, y_train, X_val, y_val, layer_dims, learning_rate, num_iterations):\n",
        "    np.random.seed(3)\n",
        "\n",
        "    W1 = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
        "    b1 = np.zeros((layer_dims[1], 1))\n",
        "\n",
        "    W2 = np.random.randn(layer_dims[2], layer_dims[1]) * 0.01\n",
        "    b2 = np.zeros((layer_dims[2], 1))\n",
        "\n",
        "    parameters = (W1, b1, W2, b2)\n",
        "\n",
        "    train_loss_list = []\n",
        "    val_loss_list = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "        # Forward Propagation\n",
        "        A2_train, cache_train = forward_propagation(X_train, parameters)\n",
        "\n",
        "        # train loss\n",
        "        train_loss = log_loss(y_train, A2_train.flatten())\n",
        "        train_loss_list.append(train_loss)\n",
        "\n",
        "        # val loss\n",
        "        A2_val, _ = forward_propagation(X_val, parameters)\n",
        "        val_loss = log_loss(y_val, A2_val.flatten())\n",
        "        val_loss_list.append(val_loss)\n",
        "\n",
        "        # 1-0\n",
        "        predictions_train = (A2_train > 0.5).astype(int)\n",
        "        predictions_val = (A2_val > 0.5).astype(int)\n",
        "\n",
        "        # Performance metrics\n",
        "        train_accuracy = accuracy_score(y_train, predictions_train.flatten())\n",
        "\n",
        "        val_accuracy = accuracy_score(y_val, predictions_val.flatten())\n",
        "\n",
        "        train_auc = roc_auc_score(y_train, A2_train.flatten())\n",
        "        val_auc = roc_auc_score(y_val, A2_val.flatten())\n",
        "\n",
        "        # Back Propagation\n",
        "        gradients = backward_propagation(X_train, y_train, cache_train, parameters)\n",
        "\n",
        "        # Update Weights\n",
        "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}, \"\n",
        "                  f\"Train Acc {train_accuracy:.4f}, Val Acc {val_accuracy:.4f}, \"\n",
        "                  f\"Train AUC {train_auc:.4f}, Val AUC {val_auc:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_loss_list, label='Training Loss')\n",
        "    plt.plot(val_loss_list, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Log Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "6SQNWbNUtJYz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Larning Processes"
      ],
      "metadata": {
        "id": "DEfLezZrtgRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(num_iterations = 1000, learning_rate = 0.01):\n",
        "  X,y = preprocess_data ('/content/diabetes.csv')\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  layer_dims = [X_train.shape[1], 10, 1]\n",
        "  parameters = train_and_evaluate(X_train, y_train, X_val, y_val, layer_dims, learning_rate, num_iterations)\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "Y8AdbdNCth3T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = main(num_iterations=10000)"
      ],
      "metadata": {
        "id": "cz6dVwu1t_Wc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}