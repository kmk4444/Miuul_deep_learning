{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOO0jZSJQ+2N8UXZhLRN+23",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/Miuul_deep_learning/blob/main/Neuralnetwork_keras_and_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Tensorflow Dataset and Modeling"
      ],
      "metadata": {
        "id": "6Xf59aL83wrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "xIJzIzzu3ygb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtA6EaU_fhjc"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "print(\"Keras Current Version:\", keras.__version__, \"Tensorflow Current Version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler # minmaxscaler alınır\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "r7IQcawJ35CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "WNi8jJXQ36-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: proprocess_data Fonksiyonunda MinMaxScaler'ı Kullanınız.\n",
        "\n",
        "- Import bölümünde MinMaxScaler'ı import etmeyi unutmayınız.\n",
        "- preprocess_data fonksiyonu içinde scaler olarak MinMaxScaler'ı kullanınız.\n",
        "- Sonrasınra train validasyon ayrımını yapınız."
      ],
      "metadata": {
        "id": "14Yaab-n373d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(filepath):\n",
        "    data = pd.read_csv(filepath)\n",
        "    scaler = MinMaxScaler() # minmaxscaler eklenir.\n",
        "    X = scaler.fit_transform(data.drop('Outcome', axis=1))\n",
        "    y = data['Outcome'].values\n",
        "    return X, y\n",
        "\n",
        "\n",
        "X, y = preprocess_data('/content/diabetes.csv')\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)"
      ],
      "metadata": {
        "id": "q_fK4JSj39HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Tensorflow Dataset\n"
      ],
      "metadata": {
        "id": "RoXs99T44ses"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Train Seti ve Validasyon Seti için Tensorflow Dataset Oluşturunuz"
      ],
      "metadata": {
        "id": "FNkLHs3A4tij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))"
      ],
      "metadata": {
        "id": "H3-G1mXg4uY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: val_dataset'in 1 Gözlemini İnceleyiniz"
      ],
      "metadata": {
        "id": "9Hi_ou2U41Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for features, label in val_dataset.take(1):\n",
        "  print('Features:', features.numpy(), 'Label:', label.numpy())"
      ],
      "metadata": {
        "id": "FZEnLUAx41zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: buffer_size'ı tüm veri seti boyutu olarak ve batch'ı 32 olacak şekilde train ve validasyon setlerini biçimlendiriniz. Validasyon seti için sadece batch uyguladığımızı unutmayın."
      ],
      "metadata": {
        "id": "-Q4WBx185CFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.shuffle(buffer_size=len(X_train)).batch(32)\n",
        "\n",
        "val_dataset = val_dataset.batch(32)"
      ],
      "metadata": {
        "id": "siTad3sy5C2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Validasyon Seti için 1 Gözlemi İnceleyiniz. 10. batch'in feature değerleri nelerdir? Label değeri nedir? Amelasyon ile tek tek sayarak inceleyiniz."
      ],
      "metadata": {
        "id": "5Nx6RnMV5-xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for features, labels in val_dataset.take(1):\n",
        "  print('Features:', features.numpy(), 'Label:', labels.numpy())"
      ],
      "metadata": {
        "id": "hKfz-P9k5_p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "igqQd5ad6krG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6: Dersteki modeli 100 epoch sayısı ile tekrar eğitiniz ve loss ve accuracy değerlerini yorumlayınız."
      ],
      "metadata": {
        "id": "-YSGelz_6lkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=100,\n",
        "                    validation_data=val_dataset)\n",
        "\n",
        "val_loss, val_acc = model.evaluate(val_dataset)\n",
        "print(\"Validation Loss:\", val_loss, \"Validation Accuracy:\", val_acc)"
      ],
      "metadata": {
        "id": "PpBkJVPF6mZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# derste elde edilen değerler\n",
        "# val_loss: 0.59\n",
        "# val_accuracy: 0.70"
      ],
      "metadata": {
        "id": "SNtBfGTC7h6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 sonucu:\n",
        "# Validation Loss: 0.5144200325012207 Validation Accuracy: 0.7532467246055603"
      ],
      "metadata": {
        "id": "JzcQL9Tw7jsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 7: 5 katmanlı ve nöron sayıları sırasıyla 32, 64, 128, 256, 512 olan bir model kurunuz ve sonuçları değerlendiriniz. Diğer özelliklerde bir değişiklik olmamalı."
      ],
      "metadata": {
        "id": "EuNX8elJ7k4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(512, activation='relu'),\n",
        "\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=100,\n",
        "                    validation_data=val_dataset)\n",
        "\n",
        "\n",
        "val_loss, val_acc = model.evaluate(val_dataset)\n",
        "print(\"Validation Loss:\", val_loss, \"Validation Accuracy:\", val_acc)"
      ],
      "metadata": {
        "id": "tWLZZI4Q7lv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 8: Acaba Epoch Sayısını Artırsak bir Faydası Olur mu? 1000 ile deneyelim. Sonuçları yorumlayalım, neden böyle?"
      ],
      "metadata": {
        "id": "c-dehb3N-hWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(512, activation='relu'),\n",
        "\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=1000,\n",
        "                    validation_data=val_dataset)\n",
        "\n",
        "\n",
        "val_loss, val_acc = model.evaluate(val_dataset)\n",
        "print(\"Validation Loss:\", val_loss, \"Validation Accuracy:\", val_acc)"
      ],
      "metadata": {
        "id": "p1cWYKNn-iV4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}